{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic batching with PyTorch (all credits to @ilblackdragon)\n",
    "\n",
    "Ref to original blog post: https://medium.com/@ilblackdragon/pytorch-dynamic-batching-f4df3dbe09ef\n",
    "\n",
    "Ref to original implementation: https://github.com/nearai/pytorch-tools/blob/master/pytorch_tools/torchfold.py\n",
    "\n",
    "Present notebook provides few examples of usage. It starts from really basic stuff. It doesn't cover technical details, for that please refer to the original article (ref above). I highly recommend also read article with DyNet implementation of the same idea (ref is in the original blog post)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torchfold import Fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Baby steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1.1: simple exponentiation\n",
    "\n",
    "The simplest example, just to show the interface of the module. Sequential calculation:\n",
    "\n",
    "![title](img/dyn_batching_simple.png)\n",
    "\n",
    "Let's calculate value of 2^n. Due to sequential nature of the task, no actual batching will be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Exp:\n",
    "    def __init__(self, base):\n",
    "        self.b = base\n",
    "    def exp(self, x):\n",
    "        return x * self.b       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fold = Fold(cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 4   # index\n",
    "exp = Exp(2)   # creating our \"network\"\n",
    "all_nodes = []  # all output nodes that we want to calculate\n",
    "result = 1   # initial value (A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building graph\n",
    "for i in range(n):\n",
    "    result = fold.add('exp',  result)\n",
    "    all_nodes.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# executing graph\n",
    "out = fold.apply(exp, [all_nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Variable containing:\n",
      "  2\n",
      "  4\n",
      "  8\n",
      " 16\n",
      "[torch.LongTensor of size 4]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(out)\n",
    "assert out[0][-1].data[0] == exp.b**n, 'ERROR!!!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1.2: exponentiation with two outputs\n",
    "\n",
    "![title](img/dyn_batching_complex.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tree:\n",
    "    def __init__(self):\n",
    "        # with input a1=1, a2=2, a3=5, expected output d=-1\n",
    "        pass\n",
    "    def B(self, x1, x2):\n",
    "        return (x1 - x2) * 2, (x1 + x2) * 3\n",
    "    def D(self, x1, x2):\n",
    "        return x1 + x2\n",
    "    def C(self, x1, x2):\n",
    "        return x1 * x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a1 = 1\n",
    "a2 = 2\n",
    "a3 = 5\n",
    "fold = Fold(cuda=False)\n",
    "bc, bd = fold.add('B', a1, a2).split(2)\n",
    "c = fold.add('C', bc, a3)\n",
    "d = fold.add('D', bd, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: -1\n"
     ]
    }
   ],
   "source": [
    "tree = Tree()\n",
    "result = fold.apply(tree, [[d]])\n",
    "print('result: {}'.format(result[0].data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. MNIST example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2.1: Simple MNIST network -- batching only forward pass\n",
    "\n",
    "Artificial task: we set batch size to 1 perform forward pass, then use TorchFold to make \"dynamic\" batching and compare times.\n",
    "\n",
    "Results on CPU: \n",
    "* 2.5s [non-batching, 10,000 examples]\n",
    "* 1.0s [batching with TorchFold, 10,000 examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download MNIST dataset\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchingNetworkForwardMNIST(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BatchingNetworkForwardMNIST, self).__init__()\n",
    "        self.lin1 = nn.Linear(784, 200)\n",
    "        self.lin2 = nn.Linear(200, 100)\n",
    "        self.logits = nn.Linear(100, 10)    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)  # reshape from (batch_size, 28, 28) to (batch_size, 784)\n",
    "        y_pred = F.relu(self.lin1(x))\n",
    "        y_pred = F.relu(self.lin2(y_pred))\n",
    "        y_pred = self.logits(y_pred)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_mnist = BatchingNetworkForwardMNIST()\n",
    "n_steps = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### non-batching example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.2 s, sys: 276 ms, total: 15.5 s\n",
      "Wall time: 2.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ma = 0.005\n",
    "running_acc = 0.0\n",
    "for i, data in enumerate(trainset):\n",
    "    inputs, labels = Variable(data[0]), Variable(torch.LongTensor([data[1]]))\n",
    "    outputs = simple_mnist(inputs)   \n",
    "    if i == n_steps:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batching example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.68 s, sys: 136 ms, total: 5.81 s\n",
      "Wall time: 970 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bs = 32\n",
    "fold = Fold()\n",
    "batch_outputs, batch_labels = [], []\n",
    "for i, data in enumerate(trainset, 0):\n",
    "    inputs, labels = Variable(data[0]), Variable(torch.Tensor([data[1]]))\n",
    "    if len(batch_outputs) < bs:\n",
    "        batch_outputs.append(fold.add('forward', inputs))        \n",
    "        batch_labels.append(labels)\n",
    "    else:\n",
    "        results = fold.apply(simple_mnist, [batch_outputs, batch_labels])\n",
    "        fold = Fold()\n",
    "        batch_outputs, batch_labels = [fold.add('forward', inputs)], [labels]\n",
    "    if i == n_steps:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2.2: Simple MNIST network -- batching forward and backward passes\n",
    "\n",
    "Artificial task: we set batch size to 1 perform training pass, then use TorchFold to make \"dynamic\" batching and compare times.\n",
    "\n",
    "Results on CPU: \n",
    "* 37.6s [non-batching, 10,000 examples]\n",
    "* 2.5s [batching with TorchFold, 10,000 examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download MNIST dataset\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchingNetworkMNIST(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BatchingNetworkMNIST, self).__init__()\n",
    "        self.lin1 = nn.Linear(784, 200)\n",
    "        self.lin2 = nn.Linear(200, 100)\n",
    "        self.logits = nn.Linear(100, 10)    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)   # reshape from (batch_size, 28, 28) to (batch_size, 784)\n",
    "        y_pred = F.relu(self.lin1(x))\n",
    "        y_pred = F.relu(self.lin2(y_pred))\n",
    "        y_pred = self.logits(y_pred)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### non-batching example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_mnist = BatchingNetworkMNIST()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(training_mnist.parameters())\n",
    "n_steps = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running acc: 47.47%\n",
      "running acc: 72.40%\n",
      "running acc: 82.94%\n",
      "running acc: 87.99%\n",
      "running acc: 89.49%\n",
      "CPU times: user 6min 49s, sys: 9.46 s, total: 6min 58s\n",
      "Wall time: 38.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ma = 0.001\n",
    "running_acc = 0.0\n",
    "for i, data in enumerate(trainset):\n",
    "    inputs, labels = Variable(data[0]), Variable(torch.LongTensor([data[1]]))\n",
    "    opt.zero_grad()\n",
    "    outputs = training_mnist(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    correct = (outputs.max(1)[1].data == labels.data).sum() / labels.data.size()[0]\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    running_acc = (1 - ma) * running_acc + ma * correct\n",
    "    if i % 1000 == 999:  # Print every 4000 mini-batches\n",
    "        print('running acc: {:.2f}%'.format(running_acc * 100))    \n",
    "    if i == n_steps:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batching example (opt step every batch_size examples, so it's not completely fair. still..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_mnist = BatchingNetworkMNIST()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(training_mnist.parameters())\n",
    "n_steps = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running acc: 37.04%\n",
      "running acc: 64.02%\n",
      "running acc: 77.65%\n",
      "running acc: 85.35%\n",
      "CPU times: user 26.9 s, sys: 748 ms, total: 27.7 s\n",
      "Wall time: 2.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bs = 32\n",
    "ma = 0.001 * bs\n",
    "running_acc = 0.0\n",
    "fold = Fold()\n",
    "batch_outputs, batch_labels = [], []\n",
    "for i, data in enumerate(trainset, 0):\n",
    "    inputs, labels = Variable(data[0]), Variable(torch.LongTensor([data[1]]))\n",
    "    if len(batch_outputs) < bs:\n",
    "        batch_outputs.append(fold.add('forward', inputs))        \n",
    "        batch_labels.append(labels)\n",
    "    else:\n",
    "        opt.zero_grad()\n",
    "        results = fold.apply(training_mnist, [batch_outputs, batch_labels])\n",
    "        loss = criterion(results[0], results[1])\n",
    "        correct = (results[0].max(1)[1].data == results[1].data).sum() / results[1].data.size()[0]\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        running_acc = (1 - ma) * running_acc + ma * correct\n",
    "        if i % 1000 < bs:  # Print every 4000 mini-batches\n",
    "            print('running acc: {:.2f}%'.format(running_acc * 100))    \n",
    "        fold = Fold()\n",
    "        batch_outputs, batch_labels = [fold.add('forward', inputs)], [labels]\n",
    "    if i == n_steps:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
